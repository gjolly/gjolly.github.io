<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Build an AI inference server on Ubuntu | Gauthier Jolly</title><meta name=keywords content="Linux,Ubuntu,AI,LLM,Docker,NVIDIA"><meta name=description content="Deploy local LLM inference with Ollama and Open WebUI"><meta name=author content="Gauthier Jolly"><link rel=canonical href=https://gjolly.fr/blog/llm-inference/><link crossorigin=anonymous href=https://gjolly.fr/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://gjolly.fr/img/terminal-sign.svg><link rel=icon type=image/png sizes=16x16 href=https://gjolly.fr/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gjolly.fr/favicon-32x32.png><link rel=apple-touch-icon href=https://gjolly.fr/apple-touch-icon.png><link rel=mask-icon href=https://gjolly.fr/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gjolly.fr/blog/llm-inference/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y4G50Q2695"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y4G50Q2695")}</script><meta property="og:url" content="https://gjolly.fr/blog/llm-inference/"><meta property="og:site_name" content="Gauthier Jolly"><meta property="og:title" content="Build an AI inference server on Ubuntu"><meta property="og:description" content="Deploy local LLM inference with Ollama and Open WebUI"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-13T19:10:00+00:00"><meta property="article:modified_time" content="2025-12-13T19:10:00+00:00"><meta property="article:tag" content="Linux"><meta property="article:tag" content="Ubuntu"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Docker"><meta property="article:tag" content="NVIDIA"><meta name=twitter:card content="summary"><meta name=twitter:title content="Build an AI inference server on Ubuntu"><meta name=twitter:description content="Deploy local LLM inference with Ollama and Open WebUI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://gjolly.fr/blog/"},{"@type":"ListItem","position":2,"name":"Build an AI inference server on Ubuntu","item":"https://gjolly.fr/blog/llm-inference/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Build an AI inference server on Ubuntu","name":"Build an AI inference server on Ubuntu","description":"Deploy local LLM inference with Ollama and Open WebUI","keywords":["Linux","Ubuntu","AI","LLM","Docker","NVIDIA"],"articleBody":"Open source tools like Ollama and Open WebUI are convenient for building local LLM inference stacks that let you create a ChatGPT-like experience on your own infrastructure. Whether you are a hobbyist, someone concerned about privacy, or a business looking to deploy LLMs on-premises, these tools can help you achieve that.\nPrerequisites We assume here that you are running an LTS version of Ubuntu (NVIDIA and AMD tooling is best supported on LTS releases) and that you have a GPU installed on your machine (either NVIDIA or AMD). If you don’t have a GPU, you can still follow this guide, but inference will be much slower as it will run on CPU.\nMaking sure the system is up-to-date As long as you use the latest kernels provided by Ubuntu, you can enjoy the pre-built NVIDIA drivers that come with the OS.\nFirst make sure your server is up-to-date:\n1 2 sudo apt update sudo apt full-upgrade -y If your system needs reboot, reboot it before running:\n1 sudo apt autoremove -y Note: You can check if your system needs to be rebooted by checking if this file exists: /var/run/reboot-required.\nRemoving old kernels is important to avoid pulling DKMS NVIDIA drivers during the installation.\nNVIDIA drivers installation (skip this step if you have an AMD GPU) Drivers Install the NVIDIA drivers by following the instructions in this post: How to install NVIDIA drivers on Ubuntu.\nNVIDIA Container Toolkit You will also need the NVIDIA container toolkit which is not available in the Ubuntu archive. Thus, you need to install the NVIDIA repository first:\n1 2 3 sudo mkdir -p /etc/apt/keyrings curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor | sudo tee /etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg \u003e /dev/null curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \u003e /dev/null Then, install the toolkit:\n1 2 sudo apt update sudo apt install -y nvidia-container-toolkit Note: You can find a detailed guide about this section on the NVIDIA documentation.\nVerify the installation You can verify that the installation was successful by running: nvidia-smi. If everything is working correctly, you should see the output of nvidia-smi showing your GPU information.\nAMD drivers installation (NVIDIA users can skip this section) Drivers The amdgpu drivers are included in the Linux kernel modules on Ubuntu and should work out of the box. To make sure you have them installed, run:\n1 apt list --installed | grep linux-modules-extra If you don’t see any output, it’s either because you are running linux-virtual (a lightweight kernel bundle for VMs) or because you are running a cloud kernel flavor that doesn’t include extra modules by default.\nIf you are on a cloud, install the appropriate extra modules package for your kernel flavor. For example, on AWS, you would run:\n1 sudo apt install -y linux-modules-extra-aws If you are not running on a cloud, install either linux-generic or linux-generic-hwe-24.04 (or -22.04 if you are using Ubuntu 22.04 LTS) depending on whether you are using the HWE kernel or not:\n1 2 3 sudo apt install -y linux-generic # or sudo apt install -y linux-generic-hwe-24.04 AMD Container Toolkit Since we’re using Docker for the LLM inference server, the ROCm toolkit (like the CUDA toolkit for NVIDIA) will be included in the container image, so there’s nothing to install.\nHowever, just like with NVIDIA, you need to configure Docker to use the AMD GPU. To do this, install the AMD container toolkit repository:\n1 2 3 4 sudo mkdir -p /etc/apt/keyrings wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | gpg --dearmor | sudo tee /etc/apt/keyrings/rocm.gpg \u003e /dev/null source /etc/os-release echo \"deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/amd-container-toolkit/apt/ ${VERSION_CODENAME} main\" | sudo tee /etc/apt/sources.list.d/amd-container-toolkit.list Then, install the toolkit:\n1 2 sudo apt update sudo apt install -y amd-container-toolkit More information can be found on the AMD ROCm documentation.\nInstalling Docker To install Docker on your machine, follow the official documentation from Docker.\nOnce done, if you are using an NVIDIA container, run the following command to configure Docker:\n1 2 sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker After running this command, you should find something like this in /etc/docker/daemon.json:\n1 2 3 4 5 6 7 8 { \"runtimes\": { \"nvidia\": { \"args\": [], \"path\": \"nvidia-container-runtime\" } } } Similarly, for AMD GPUs, run:\n1 2 sudo amd-ctk runtime configure sudo systemctl restart docker and you should find something like this in /etc/docker/daemon.json:\n1 2 3 4 5 6 7 8 { \"runtimes\": { \"amd\": { \"path\": \"amd-container-runtime\", \"runtimeArgs\": [] } } } Installing Ollama and Open WebUI Ollama is the server that will be running the LLMs and Open WebUI is the ChatGPT-like UI to chat with the model.\nCreate a compose.yml file with the following content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 services: ollama: # use ollama/ollama:rocm for AMD GPU image: ollama/ollama volumes: - ollama:/root/.ollama container_name: ollama pull_policy: always tty: true restart: unless-stopped # set \"amd\" for AMD GPU runtime: nvidia gpus: all open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui volumes: - openwebui:/app/backend/data pull_policy: always depends_on: - ollama ports: - 127.0.0.1:8080:8080 environment: OLLAMA_BASE_URL: http://ollama:11434 WEBUI_URL: http://localhost:8080 restart: unless-stopped volumes: ollama: openwebui: Simply run docker compose up -d, and you should be able to open http://localhost:8080 in your favorite web browser and start chatting with your model!\nBut wait, you don’t have any model yet!\nDownloading a model You can download models directly from the ollama container. For example, to download the llama2 model, run:\n1 docker compose exec -it ollama ollama pull llama2 This will download the model inside the ollama container and make it available for inference.\nYou can also list available models by running:\n1 docker compose exec -it ollama ollama list or check the Ollama model repository for more models. Make sure the size of the model fits in your GPU memory! For example, llama2 requires at least 4GB of GPU memory. You can check your GPU memory by running nvidia-smi or btop.\nNote: The first time a model is used, it might take a bit longer to respond as it needs to be loaded into GPU memory.\nMaintenance To update the Ollama and Open WebUI images, simply run:\n1 2 docker compose pull docker compose up -d To keep the NVIDIA drivers up-to-date and never pull the DKMS packages, follow the instructions in this post: How to install NVIDIA drivers on Ubuntu.\nTroubleshooting I find that the best way to monitor the GPU usage is to use btop. If you have nvidia-smi or rocm-smi installed, btop will show you the GPU usage in its UI.\nOne of the first things to check if you think that the GPU is not being used is the logs from the ollama container:\n1 docker compose logs -f ollama and to look for something like:\nlevel=INFO source=types.go:42 msg=\"inference compute\" id=GPU-8c5284c3-6336-84e6-f91e-ba027e8d440b filter_id=\"\" library=CUDA compute=8.9 name=CUDA0 description=\"NVIDIA L4\" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:31:00.0 type=discrete total=\"22.5 GiB\" available=\"22.0 GiB\" [...] llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L4) (0000:31:00.0) - 22560 MiB free [...] load_tensors: offloading 32 repeating layers to GPU If you see that the model is being loaded on CPU instead of GPU, then there is probably something wrong with your NVIDIA or AMD container toolkit or drivers installation.\nCheck that your card is visible either by running nvidia-smi, rocm-smi or btop on the host machine. If it is not, then the problem is with your drivers installation.\nReferences Ollama documentation Open-webui documentation NVIDIA Container Toolkit installation guide Ubuntu Kernel cycles ","wordCount":"1252","inLanguage":"en","datePublished":"2025-12-13T19:10:00Z","dateModified":"2025-12-13T19:10:00Z","author":{"@type":"Person","name":"Gauthier Jolly"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gjolly.fr/blog/llm-inference/"},"publisher":{"@type":"Organization","name":"Gauthier Jolly","logo":{"@type":"ImageObject","url":"https://gjolly.fr/img/terminal-sign.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gjolly.fr/ accesskey=h title="Home (Alt + H)"><img src=https://gjolly.fr/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gjolly.fr/blog/ title=blog><span>blog</span></a></li><li><a href=https://gjolly.fr/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gjolly.fr/>Home</a>&nbsp;»&nbsp;<a href=https://gjolly.fr/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">Build an AI inference server on Ubuntu</h1><div class=post-description>Deploy local LLM inference with Ollama and Open WebUI</div><div class=post-meta><span title='2025-12-13 19:10:00 +0000 UTC'>December 13, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Gauthier Jolly</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#making-sure-the-system-is-up-to-date>Making sure the system is up-to-date</a></li><li><a href=#nvidia-drivers-installation-skip-this-step-if-you-have-an-amd-gpu>NVIDIA drivers installation (skip this step if you have an AMD GPU)</a><ul><li><a href=#drivers>Drivers</a></li><li><a href=#nvidia-container-toolkit>NVIDIA Container Toolkit</a></li><li><a href=#verify-the-installation>Verify the installation</a></li></ul></li><li><a href=#amd-drivers-installation-nvidia-users-can-skip-this-section>AMD drivers installation (NVIDIA users can skip this section)</a><ul><li><a href=#drivers-1>Drivers</a></li><li><a href=#amd-container-toolkit>AMD Container Toolkit</a></li></ul></li><li><a href=#installing-docker>Installing Docker</a></li><li><a href=#installing-ollama-and-open-webui>Installing Ollama and Open WebUI</a><ul><li><a href=#downloading-a-model>Downloading a model</a></li></ul></li><li><a href=#maintenance>Maintenance</a></li><li><a href=#troubleshooting>Troubleshooting</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>Open source tools like <a href=https://ollama.com/>Ollama</a> and <a href=https://docs.openwebui.com/>Open WebUI</a> are convenient for building local LLM inference stacks that let you create a ChatGPT-like experience on your own infrastructure. Whether you are a hobbyist, someone concerned about privacy, or a business looking to deploy LLMs on-premises, these tools can help you achieve that.</p><h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2><p>We assume here that you are running an LTS version of Ubuntu (NVIDIA and AMD tooling is best supported on LTS releases) and that you have a GPU installed on your machine (either NVIDIA or AMD). If you don&rsquo;t have a GPU, you can still follow this guide, but inference will be much slower as it will run on CPU.</p><h2 id=making-sure-the-system-is-up-to-date>Making sure the system is up-to-date<a hidden class=anchor aria-hidden=true href=#making-sure-the-system-is-up-to-date>#</a></h2><p>As long as you use the latest kernels provided by Ubuntu, you can enjoy the pre-built NVIDIA drivers that come with the OS.</p><p>First make sure your server is up-to-date:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt update
</span></span><span class=line><span class=cl>sudo apt full-upgrade -y
</span></span></code></pre></td></tr></table></div></div><p>If your system needs reboot, reboot it before running:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt autoremove -y
</span></span></code></pre></td></tr></table></div></div><blockquote><p><strong>Note</strong>: You can check if your system needs to be rebooted by checking if this file exists: <code>/var/run/reboot-required</code>.</p></blockquote><p>Removing old kernels is important to avoid pulling DKMS NVIDIA drivers during the installation.</p><h2 id=nvidia-drivers-installation-skip-this-step-if-you-have-an-amd-gpu>NVIDIA drivers installation (skip this step if you have an AMD GPU)<a hidden class=anchor aria-hidden=true href=#nvidia-drivers-installation-skip-this-step-if-you-have-an-amd-gpu>#</a></h2><h3 id=drivers>Drivers<a hidden class=anchor aria-hidden=true href=#drivers>#</a></h3><p>Install the NVIDIA drivers by following the instructions in this post: <a href=https://gjolly.fr/blog/nivdia-drivers/>How to install NVIDIA drivers on Ubuntu</a>.</p><h3 id=nvidia-container-toolkit>NVIDIA Container Toolkit<a hidden class=anchor aria-hidden=true href=#nvidia-container-toolkit>#</a></h3><p>You will also need the NVIDIA container toolkit which is not available in the Ubuntu archive. Thus, you need to install the NVIDIA repository first:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo mkdir -p /etc/apt/keyrings
</span></span><span class=line><span class=cl>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey <span class=p>|</span> gpg --dearmor <span class=p>|</span> sudo tee /etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg &gt; /dev/null
</span></span><span class=line><span class=cl>curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list <span class=p>|</span> sed <span class=s1>&#39;s#deb https://#deb [signed-by=/etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> <span class=p>|</span> sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list &gt; /dev/null
</span></span></code></pre></td></tr></table></div></div><p>Then, install the toolkit:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1>1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt update
</span></span><span class=line><span class=cl>sudo apt install -y nvidia-container-toolkit
</span></span></code></pre></td></tr></table></div></div><blockquote><p><strong>Note</strong>: You can find a detailed guide about this section on the <a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html>NVIDIA documentation</a>.</p></blockquote><h3 id=verify-the-installation>Verify the installation<a hidden class=anchor aria-hidden=true href=#verify-the-installation>#</a></h3><p>You can verify that the installation was successful by running: <code>nvidia-smi</code>.
If everything is working correctly, you should see the output of <code>nvidia-smi</code> showing your GPU information.</p><h2 id=amd-drivers-installation-nvidia-users-can-skip-this-section>AMD drivers installation (NVIDIA users can skip this section)<a hidden class=anchor aria-hidden=true href=#amd-drivers-installation-nvidia-users-can-skip-this-section>#</a></h2><h3 id=drivers-1>Drivers<a hidden class=anchor aria-hidden=true href=#drivers-1>#</a></h3><p>The <code>amdgpu</code> drivers are included in the Linux kernel modules on Ubuntu and should work out of the box. To make sure you have them installed, run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>apt list --installed <span class=p>|</span> grep linux-modules-extra
</span></span></code></pre></td></tr></table></div></div><p>If you don&rsquo;t see any output, it&rsquo;s either because you are running <code>linux-virtual</code> (a lightweight kernel bundle for VMs) or because you are running a cloud kernel flavor that doesn&rsquo;t include extra modules by default.</p><p>If you are on a cloud, install the appropriate extra modules package for your kernel flavor. For example, on AWS, you would run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt install -y linux-modules-extra-aws
</span></span></code></pre></td></tr></table></div></div><p>If you are not running on a cloud, install either <code>linux-generic</code> or <code>linux-generic-hwe-24.04</code> (or <code>-22.04</code> if you are using Ubuntu 22.04 LTS) depending on whether you are using the HWE kernel or not:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1>1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2>2</a>
</span><span class=lnt id=hl-6-3><a class=lnlinks href=#hl-6-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt install -y linux-generic
</span></span><span class=line><span class=cl><span class=c1># or</span>
</span></span><span class=line><span class=cl>sudo apt install -y linux-generic-hwe-24.04
</span></span></code></pre></td></tr></table></div></div><h3 id=amd-container-toolkit>AMD Container Toolkit<a hidden class=anchor aria-hidden=true href=#amd-container-toolkit>#</a></h3><p>Since we&rsquo;re using Docker for the LLM inference server, the ROCm toolkit (like the CUDA toolkit for NVIDIA) will be included in the container image, so there&rsquo;s nothing to install.</p><p>However, just like with NVIDIA, you need to configure Docker to use the AMD GPU. To do this, install the AMD container toolkit repository:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1>1</a>
</span><span class=lnt id=hl-7-2><a class=lnlinks href=#hl-7-2>2</a>
</span><span class=lnt id=hl-7-3><a class=lnlinks href=#hl-7-3>3</a>
</span><span class=lnt id=hl-7-4><a class=lnlinks href=#hl-7-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo mkdir -p /etc/apt/keyrings
</span></span><span class=line><span class=cl>wget https://repo.radeon.com/rocm/rocm.gpg.key -O - <span class=p>|</span> gpg --dearmor <span class=p>|</span> sudo tee /etc/apt/keyrings/rocm.gpg &gt; /dev/null
</span></span><span class=line><span class=cl><span class=nb>source</span> /etc/os-release
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/amd-container-toolkit/apt/ </span><span class=si>${</span><span class=nv>VERSION_CODENAME</span><span class=si>}</span><span class=s2> main&#34;</span> <span class=p>|</span> sudo tee /etc/apt/sources.list.d/amd-container-toolkit.list
</span></span></code></pre></td></tr></table></div></div><p>Then, install the toolkit:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-8-1><a class=lnlinks href=#hl-8-1>1</a>
</span><span class=lnt id=hl-8-2><a class=lnlinks href=#hl-8-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt update
</span></span><span class=line><span class=cl>sudo apt install -y amd-container-toolkit
</span></span></code></pre></td></tr></table></div></div><p>More information can be found on the <a href=https://instinct.docs.amd.com/projects/container-toolkit/en/latest/container-runtime/quick-start-guide.html#step-3-configure-repositories>AMD ROCm documentation</a>.</p><h2 id=installing-docker>Installing Docker<a hidden class=anchor aria-hidden=true href=#installing-docker>#</a></h2><p>To install Docker on your machine, follow <a href=https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository>the official documentation from Docker</a>.</p><p>Once done, if you are using an NVIDIA container, run the following command to configure Docker:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-9-1><a class=lnlinks href=#hl-9-1>1</a>
</span><span class=lnt id=hl-9-2><a class=lnlinks href=#hl-9-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo nvidia-ctk runtime configure --runtime<span class=o>=</span>docker
</span></span><span class=line><span class=cl>sudo systemctl restart docker
</span></span></code></pre></td></tr></table></div></div><p>After running this command, you should find something like this in <code>/etc/docker/daemon.json</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-10-1><a class=lnlinks href=#hl-10-1>1</a>
</span><span class=lnt id=hl-10-2><a class=lnlinks href=#hl-10-2>2</a>
</span><span class=lnt id=hl-10-3><a class=lnlinks href=#hl-10-3>3</a>
</span><span class=lnt id=hl-10-4><a class=lnlinks href=#hl-10-4>4</a>
</span><span class=lnt id=hl-10-5><a class=lnlinks href=#hl-10-5>5</a>
</span><span class=lnt id=hl-10-6><a class=lnlinks href=#hl-10-6>6</a>
</span><span class=lnt id=hl-10-7><a class=lnlinks href=#hl-10-7>7</a>
</span><span class=lnt id=hl-10-8><a class=lnlinks href=#hl-10-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;runtimes&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;nvidia&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;args&#34;</span><span class=p>:</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;path&#34;</span><span class=p>:</span> <span class=s2>&#34;nvidia-container-runtime&#34;</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Similarly, for AMD GPUs, run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-11-1><a class=lnlinks href=#hl-11-1>1</a>
</span><span class=lnt id=hl-11-2><a class=lnlinks href=#hl-11-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo amd-ctk runtime configure
</span></span><span class=line><span class=cl>sudo systemctl restart docker
</span></span></code></pre></td></tr></table></div></div><p>and you should find something like this in <code>/etc/docker/daemon.json</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-12-1><a class=lnlinks href=#hl-12-1>1</a>
</span><span class=lnt id=hl-12-2><a class=lnlinks href=#hl-12-2>2</a>
</span><span class=lnt id=hl-12-3><a class=lnlinks href=#hl-12-3>3</a>
</span><span class=lnt id=hl-12-4><a class=lnlinks href=#hl-12-4>4</a>
</span><span class=lnt id=hl-12-5><a class=lnlinks href=#hl-12-5>5</a>
</span><span class=lnt id=hl-12-6><a class=lnlinks href=#hl-12-6>6</a>
</span><span class=lnt id=hl-12-7><a class=lnlinks href=#hl-12-7>7</a>
</span><span class=lnt id=hl-12-8><a class=lnlinks href=#hl-12-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;runtimes&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;amd&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;path&#34;</span><span class=p>:</span> <span class=s2>&#34;amd-container-runtime&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;runtimeArgs&#34;</span><span class=p>:</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=installing-ollama-and-open-webui>Installing Ollama and Open WebUI<a hidden class=anchor aria-hidden=true href=#installing-ollama-and-open-webui>#</a></h2><p>Ollama is the server that will be running the LLMs and Open WebUI is the ChatGPT-like UI to chat with the model.</p><p>Create a <code>compose.yml</code> file with the following content:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-13-1><a class=lnlinks href=#hl-13-1> 1</a>
</span><span class=lnt id=hl-13-2><a class=lnlinks href=#hl-13-2> 2</a>
</span><span class=lnt id=hl-13-3><a class=lnlinks href=#hl-13-3> 3</a>
</span><span class=lnt id=hl-13-4><a class=lnlinks href=#hl-13-4> 4</a>
</span><span class=lnt id=hl-13-5><a class=lnlinks href=#hl-13-5> 5</a>
</span><span class=lnt id=hl-13-6><a class=lnlinks href=#hl-13-6> 6</a>
</span><span class=lnt id=hl-13-7><a class=lnlinks href=#hl-13-7> 7</a>
</span><span class=lnt id=hl-13-8><a class=lnlinks href=#hl-13-8> 8</a>
</span><span class=lnt id=hl-13-9><a class=lnlinks href=#hl-13-9> 9</a>
</span><span class=lnt id=hl-13-10><a class=lnlinks href=#hl-13-10>10</a>
</span><span class=lnt id=hl-13-11><a class=lnlinks href=#hl-13-11>11</a>
</span><span class=lnt id=hl-13-12><a class=lnlinks href=#hl-13-12>12</a>
</span><span class=lnt id=hl-13-13><a class=lnlinks href=#hl-13-13>13</a>
</span><span class=lnt id=hl-13-14><a class=lnlinks href=#hl-13-14>14</a>
</span><span class=lnt id=hl-13-15><a class=lnlinks href=#hl-13-15>15</a>
</span><span class=lnt id=hl-13-16><a class=lnlinks href=#hl-13-16>16</a>
</span><span class=lnt id=hl-13-17><a class=lnlinks href=#hl-13-17>17</a>
</span><span class=lnt id=hl-13-18><a class=lnlinks href=#hl-13-18>18</a>
</span><span class=lnt id=hl-13-19><a class=lnlinks href=#hl-13-19>19</a>
</span><span class=lnt id=hl-13-20><a class=lnlinks href=#hl-13-20>20</a>
</span><span class=lnt id=hl-13-21><a class=lnlinks href=#hl-13-21>21</a>
</span><span class=lnt id=hl-13-22><a class=lnlinks href=#hl-13-22>22</a>
</span><span class=lnt id=hl-13-23><a class=lnlinks href=#hl-13-23>23</a>
</span><span class=lnt id=hl-13-24><a class=lnlinks href=#hl-13-24>24</a>
</span><span class=lnt id=hl-13-25><a class=lnlinks href=#hl-13-25>25</a>
</span><span class=lnt id=hl-13-26><a class=lnlinks href=#hl-13-26>26</a>
</span><span class=lnt id=hl-13-27><a class=lnlinks href=#hl-13-27>27</a>
</span><span class=lnt id=hl-13-28><a class=lnlinks href=#hl-13-28>28</a>
</span><span class=lnt id=hl-13-29><a class=lnlinks href=#hl-13-29>29</a>
</span><span class=lnt id=hl-13-30><a class=lnlinks href=#hl-13-30>30</a>
</span><span class=lnt id=hl-13-31><a class=lnlinks href=#hl-13-31>31</a>
</span><span class=lnt id=hl-13-32><a class=lnlinks href=#hl-13-32>32</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ollama</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c># use ollama/ollama:rocm for AMD GPU</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>ollama/ollama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>ollama:/root/.ollama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>ollama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>pull_policy</span><span class=p>:</span><span class=w> </span><span class=l>always</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tty</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c># set &#34;amd&#34; for AMD GPU</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runtime</span><span class=p>:</span><span class=w> </span><span class=l>nvidia</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>gpus</span><span class=p>:</span><span class=w> </span><span class=l>all</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>open-webui</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>ghcr.io/open-webui/open-webui:main</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>container_name</span><span class=p>:</span><span class=w> </span><span class=l>open-webui</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>openwebui:/app/backend/data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>pull_policy</span><span class=p>:</span><span class=w> </span><span class=l>always</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>ollama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=m>127.0.0.1</span><span class=p>:</span><span class=m>8080</span><span class=p>:</span><span class=m>8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>OLLAMA_BASE_URL</span><span class=p>:</span><span class=w> </span><span class=l>http://ollama:11434</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>WEBUI_URL</span><span class=p>:</span><span class=w> </span><span class=l>http://localhost:8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>restart</span><span class=p>:</span><span class=w> </span><span class=l>unless-stopped</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ollama</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>openwebui</span><span class=p>:</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>Simply run <code>docker compose up -d</code>, and you should be able to open <a href=http://localhost:8080>http://localhost:8080</a> in your favorite web browser and start chatting with your model!</p><p>But wait, you don&rsquo;t have any model yet!</p><h3 id=downloading-a-model>Downloading a model<a hidden class=anchor aria-hidden=true href=#downloading-a-model>#</a></h3><p>You can download models directly from the <code>ollama</code> container. For example, to download the <code>llama2</code> model, run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-14-1><a class=lnlinks href=#hl-14-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker compose <span class=nb>exec</span> -it ollama ollama pull llama2
</span></span></code></pre></td></tr></table></div></div><p>This will download the model inside the <code>ollama</code> container and make it available for inference.</p><p>You can also list available models by running:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-15-1><a class=lnlinks href=#hl-15-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker compose <span class=nb>exec</span> -it ollama ollama list
</span></span></code></pre></td></tr></table></div></div><p>or check the <a href=https://ollama.com/models>Ollama model repository</a> for more models. <strong>Make sure the size of the model fits in your GPU memory!</strong> For example, <code>llama2</code> requires at least 4GB of GPU memory. You can check your GPU memory by running <code>nvidia-smi</code> or <code>btop</code>.</p><blockquote><p><strong>Note</strong>: The first time a model is used, it might take a bit longer to respond as it needs to be loaded into GPU memory.</p></blockquote><h2 id=maintenance>Maintenance<a hidden class=anchor aria-hidden=true href=#maintenance>#</a></h2><p>To update the Ollama and Open WebUI images, simply run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-16-1><a class=lnlinks href=#hl-16-1>1</a>
</span><span class=lnt id=hl-16-2><a class=lnlinks href=#hl-16-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker compose pull
</span></span><span class=line><span class=cl>docker compose up -d
</span></span></code></pre></td></tr></table></div></div><p>To keep the NVIDIA drivers up-to-date and never pull the DKMS packages, follow the instructions in this post: <a href=https://gjolly.fr/blog/nivdia-drivers/>How to install NVIDIA drivers on Ubuntu</a>.</p><h2 id=troubleshooting>Troubleshooting<a hidden class=anchor aria-hidden=true href=#troubleshooting>#</a></h2><p>I find that the best way to monitor the GPU usage is to use <code>btop</code>. If you have <code>nvidia-smi</code> or <code>rocm-smi</code> installed, <code>btop</code> will show you the GPU usage in its UI.</p><p>One of the first things to check if you think that the GPU is not being used is the logs from the <code>ollama</code> container:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-17-1><a class=lnlinks href=#hl-17-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker compose logs -f ollama
</span></span></code></pre></td></tr></table></div></div><p>and to look for something like:</p><pre tabindex=0><code>level=INFO source=types.go:42 msg=&#34;inference compute&#34; id=GPU-8c5284c3-6336-84e6-f91e-ba027e8d440b filter_id=&#34;&#34; library=CUDA compute=8.9 name=CUDA0 description=&#34;NVIDIA L4&#34; libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:31:00.0 type=discrete total=&#34;22.5 GiB&#34; available=&#34;22.0 GiB&#34;
[...]
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA L4) (0000:31:00.0) - 22560 MiB free
[...]
load_tensors: offloading 32 repeating layers to GPU
</code></pre><p>If you see that the model is being loaded on CPU instead of GPU, then there is probably something wrong with your NVIDIA or AMD container toolkit or drivers installation.</p><p>Check that your card is visible either by running <code>nvidia-smi</code>, <code>rocm-smi</code> or <code>btop</code> on the host machine. If it is not, then the problem is with your drivers installation.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://docs.ollama.com/docker>Ollama documentation</a></li><li><a href=https://docs.openwebui.com/>Open-webui documentation</a></li><li><a href=https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html>NVIDIA Container Toolkit installation guide</a></li><li><a href="https://ubuntu.com/about/release-cycle?product=ubuntu-kernel&amp;release=ubuntu+kernel&amp;version=all">Ubuntu Kernel cycles</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://gjolly.fr/tags/linux/>Linux</a></li><li><a href=https://gjolly.fr/tags/ubuntu/>Ubuntu</a></li><li><a href=https://gjolly.fr/tags/ai/>AI</a></li><li><a href=https://gjolly.fr/tags/llm/>LLM</a></li><li><a href=https://gjolly.fr/tags/docker/>Docker</a></li><li><a href=https://gjolly.fr/tags/nvidia/>NVIDIA</a></li></ul><nav class=paginav><a class=prev href=https://gjolly.fr/blog/cf-tunnels/><span class=title>« Prev</span><br><span>Exposing a local web server using Cloudflare Tunnels</span>
</a><a class=next href=https://gjolly.fr/blog/ubuntu-desktop-images/><span class=title>Next »</span><br><span>Build an Ubuntu Destkop image with genesis</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://gjolly.fr/>Gauthier Jolly</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>